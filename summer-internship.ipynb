{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1845144,"sourceType":"datasetVersion","datasetId":1097190},{"sourceId":7453063,"sourceType":"datasetVersion","datasetId":4338020},{"sourceId":9781126,"sourceType":"datasetVersion","datasetId":5992294},{"sourceId":9781134,"sourceType":"datasetVersion","datasetId":5992299},{"sourceId":9782812,"sourceType":"datasetVersion","datasetId":5993515},{"sourceId":9782875,"sourceType":"datasetVersion","datasetId":5993564},{"sourceId":9782928,"sourceType":"datasetVersion","datasetId":5993608}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-11-01T12:32:07.396419Z","iopub.execute_input":"2024-11-01T12:32:07.396738Z","iopub.status.idle":"2024-11-01T12:32:08.560365Z","shell.execute_reply.started":"2024-11-01T12:32:07.396705Z","shell.execute_reply":"2024-11-01T12:32:08.559413Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ShEIsOpbAYmQL7wFC52jrk_qJ2UKB_B-' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1ShEIsOpbAYmQL7wFC52jrk_qJ2UKB_B-\" -O Dataset.zip && rm -rf /tmp/cookies.txt\n!unzip Dataset.zip -d Dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T10:42:46.708623Z","iopub.execute_input":"2024-11-01T10:42:46.709160Z","iopub.status.idle":"2024-11-01T10:42:49.929878Z","shell.execute_reply.started":"2024-11-01T10:42:46.709123Z","shell.execute_reply":"2024-11-01T10:42:49.928673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pafy youtube-dl moviepy","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:22:19.417531Z","iopub.execute_input":"2024-11-01T18:22:19.418164Z","iopub.status.idle":"2024-11-01T18:22:49.493702Z","shell.execute_reply.started":"2024-11-01T18:22:19.418125Z","shell.execute_reply":"2024-11-01T18:22:49.492602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport cv2\nimport pafy\nimport math\nimport random\nimport numpy as np\nimport datetime as dt\nimport tensorflow as tf\nfrom collections import deque\nimport matplotlib.pyplot as plt\n\nfrom moviepy.editor import *\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:22:49.496156Z","iopub.execute_input":"2024-11-01T18:22:49.496576Z","iopub.status.idle":"2024-11-01T18:23:02.655640Z","shell.execute_reply.started":"2024-11-01T18:22:49.496530Z","shell.execute_reply":"2024-11-01T18:23:02.654833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_constant = 5\nnp.random.seed(seed_constant)\nrandom.seed(seed_constant)\ntf.random.set_seed(seed_constant)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:23:02.656759Z","iopub.execute_input":"2024-11-01T18:23:02.657274Z","iopub.status.idle":"2024-11-01T18:23:02.662165Z","shell.execute_reply.started":"2024-11-01T18:23:02.657239Z","shell.execute_reply":"2024-11-01T18:23:02.661143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set Kaggle dataset path\nDataset = '/kaggle/input/dataset'","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:23:20.057499Z","iopub.execute_input":"2024-11-01T18:23:20.058647Z","iopub.status.idle":"2024-11-01T18:23:20.063960Z","shell.execute_reply.started":"2024-11-01T18:23:20.058578Z","shell.execute_reply":"2024-11-01T18:23:20.062869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport cv2\nimport matplotlib.pyplot as plt\n\n# Set the path to the base directory containing 'fights' and 'noFights' subdirectories\nbase_directory = '/kaggle/input/dataset'\n\n# Create a Matplotlib figure and specify the size of the figure\nplt.figure(figsize=(20, 20))\n\n# Get the names of all classes/categories in the dataset\nall_classes_names = os.listdir(base_directory)\n\n# Generate a list of 20 random values based on the number of classes\nrandom_classes = random.choices(all_classes_names, k=20)\n\n# Iterating through the randomly selected class names\nfor counter, selected_class_Name in enumerate(random_classes, 1):\n    try:\n        # Retrieve the list of all the video files present in the selected Class Directory\n        video_files_names_list = os.listdir(f'{base_directory}/{selected_class_Name}')\n        \n        # Randomly select a video file from the list\n        selected_video_file_name = random.choice(video_files_names_list)\n        \n        # Initialize a VideoCapture object to read from the video file\n        video_reader = cv2.VideoCapture(f'{base_directory}/{selected_class_Name}/{selected_video_file_name}')\n        video_reader.set(1, 25)  # Set to a frame position\n        \n        # Read the specified frame of the video file\n        success, bgr_frame = video_reader.read()\n        \n        # If frame is read successfully, process it\n        if success:\n            bgr_frame = cv2.resize(bgr_frame, (224, 224))\n            video_reader.release()\n            \n            # Convert the frame from BGR to RGB format\n            rgb_frame = cv2.cvtColor(bgr_frame, cv2.COLOR_BGR2RGB)\n            \n            # Write the class name on the video frame\n            cv2.putText(rgb_frame, selected_class_Name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 200, 255), 2)\n            \n            # Display the frame\n            plt.subplot(5, 4, counter)\n            plt.imshow(rgb_frame)\n            plt.axis('off')\n        else:\n            print(f\"Could not read frame from {selected_video_file_name}\")\n            video_reader.release()\n\n    except FileNotFoundError as e:\n        print(f\"Error: {e}. Directory might not exist: {base_directory}/{selected_class_Name}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n\n# Show the plot at the end\nplt.tight_layout()\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:23:29.335625Z","iopub.execute_input":"2024-11-01T18:23:29.336270Z","iopub.status.idle":"2024-11-01T18:23:32.773490Z","shell.execute_reply.started":"2024-11-01T18:23:29.336231Z","shell.execute_reply":"2024-11-01T18:23:32.771971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Specify the height and width to which each video frame will be resized in our dataset.\nIMAGE_HEIGHT , IMAGE_WIDTH = 64, 64\n \n# Specify the number of frames of a video that will be fed to the model as one sequence.\nSEQUENCE_LENGTH = 30\n \n# Specify the directory containing the UCF50 dataset. \nDATASET_DIR = \"/kaggle/input/dataset\"\n \n# Specify the list containing the names of the classes used for training. Feel free to choose any set of classes.\nCLASSES_LIST = [\"walking\", \"fights\", \"running\",\"noFights\"]","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:24:32.563009Z","iopub.execute_input":"2024-11-01T18:24:32.564062Z","iopub.status.idle":"2024-11-01T18:24:32.568793Z","shell.execute_reply.started":"2024-11-01T18:24:32.564016Z","shell.execute_reply":"2024-11-01T18:24:32.567811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def frames_extraction(video_path):\n    '''\n    This function will extract the required frames from a video after resizing and normalizing them.\n    Args:\n        video_path: The path of the video in the disk, whose frames are to be extracted.\n    Returns:\n        frames_list: A list containing the resized and normalized frames of the video.\n    '''\n\n    # Declare a list to store video frames.\n    frames_list = []\n    \n    # Read the Video File using the VideoCapture object.\n    video_reader = cv2.VideoCapture(video_path)\n\n    # Get the total number of frames in the video.\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Calculate the the interval after which frames will be added to the list.\n    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH), 1)\n\n    # Iterate through the Video Frames.\n    for frame_counter in range(SEQUENCE_LENGTH):\n\n        # Set the current frame position of the video.\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n\n        # Reading the frame from the video. \n        success, frame = video_reader.read() \n\n        # Check if Video frame is not successfully read then break the loop\n        if not success:\n            break\n\n        # Resize the Frame to fixed height and width.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        \n        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n        normalized_frame = resized_frame / 255\n        \n        # Append the normalized frame into the frames list\n        frames_list.append(normalized_frame)\n    \n    # Release the VideoCapture object. \n    video_reader.release()\n\n    # Return the frames list.\n    return frames_list","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:24:36.592995Z","iopub.execute_input":"2024-11-01T18:24:36.593394Z","iopub.status.idle":"2024-11-01T18:24:36.602004Z","shell.execute_reply.started":"2024-11-01T18:24:36.593355Z","shell.execute_reply":"2024-11-01T18:24:36.600922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_dataset():\n    '''\n    This function will extract the data of the selected classes and create the required dataset.\n    Returns:\n        features:          A list containing the extracted frames of the videos.\n        labels:            A list containing the indexes of the classes associated with the videos.\n        video_files_paths: A list containing the paths of the videos in the disk.\n    '''\n\n    # Declared Empty Lists to store the features, labels and video file path values.\n    features = []\n    labels = []\n    video_files_paths = []\n    \n    # Iterating through all the classes mentioned in the classes list\n    for class_index, class_name in enumerate(CLASSES_LIST):\n        \n        # Display the name of the class whose data is being extracted.\n        print(f'Extracting Data of Class: {class_name}')\n        \n        # Get the list of video files present in the specific class name directory.\n        files_list = os.listdir(os.path.join(DATASET_DIR, class_name))\n        \n        # Iterate through all the files present in the files list.\n        for file_name in files_list:\n            \n            # Get the complete video path.\n            video_file_path = os.path.join(DATASET_DIR, class_name, file_name)\n\n            # Extract the frames of the video file.\n            frames = frames_extraction(video_file_path)\n\n            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n            if len(frames) == SEQUENCE_LENGTH:\n\n                # Append the data to their repective lists.\n                features.append(frames)\n                labels.append(class_index)\n                video_files_paths.append(video_file_path)\n\n    # Converting the list to numpy arrays\n    features = np.asarray(features)\n    labels = np.array(labels)  \n    \n    # Return the frames, class index, and video file path.\n    return features, labels, video_files_paths","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:24:37.802104Z","iopub.execute_input":"2024-11-01T18:24:37.802465Z","iopub.status.idle":"2024-11-01T18:24:37.810991Z","shell.execute_reply.started":"2024-11-01T18:24:37.802432Z","shell.execute_reply":"2024-11-01T18:24:37.810058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the dataset.\nfeatures, labels, video_files_paths = create_dataset()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:24:39.416552Z","iopub.execute_input":"2024-11-01T18:24:39.417465Z","iopub.status.idle":"2024-11-01T18:26:36.921005Z","shell.execute_reply.started":"2024-11-01T18:24:39.417422Z","shell.execute_reply":"2024-11-01T18:26:36.920105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Keras's to_categorical method to convert labels into one-hot-encoded vectors\none_hot_encoded_labels = to_categorical(labels)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:26:48.328497Z","iopub.execute_input":"2024-11-01T18:26:48.329143Z","iopub.status.idle":"2024-11-01T18:26:48.333723Z","shell.execute_reply.started":"2024-11-01T18:26:48.329103Z","shell.execute_reply":"2024-11-01T18:26:48.332737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_train, features_test, labels_train, labels_test = train_test_split(features, one_hot_encoded_labels, test_size = 0.25, shuffle = True, random_state = seed_constant)\nfeatures = None\nlabels = None","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:26:51.166064Z","iopub.execute_input":"2024-11-01T18:26:51.166885Z","iopub.status.idle":"2024-11-01T18:26:51.517933Z","shell.execute_reply.started":"2024-11-01T18:26:51.166847Z","shell.execute_reply":"2024-11-01T18:26:51.517148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_LRCN_model():\n    '''\n    This function will construct the required LRCN model.\n    Returns:\n        model: It is the required constructed LRCN model.\n    '''\n\n    # We will use a Sequential model for model construction.\n    model = Sequential()\n    \n    # Define the Model Architecture.\n    ########################################################################################################################\n    \n    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same',activation = 'relu'), input_shape = (SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n    \n    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same',activation = 'relu')))\n    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n    \n    model.add(TimeDistributed(Conv2D(128, (3, 3), padding='same',activation = 'relu')))\n    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n    \n    model.add(TimeDistributed(Conv2D(256, (2, 2), padding='same',activation = 'relu')))\n    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n                                      \n    model.add(TimeDistributed(Flatten()))\n                                      \n    model.add(LSTM(32))\n                                      \n    model.add(Dense(len(CLASSES_LIST), activation = 'softmax'))\n\n    ########################################################################################################################\n\n    # Display the models summary.\n    model.summary()\n    \n    # Return the constructed LRCN model.\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:26:53.297057Z","iopub.execute_input":"2024-11-01T18:26:53.297739Z","iopub.status.idle":"2024-11-01T18:26:53.307316Z","shell.execute_reply.started":"2024-11-01T18:26:53.297700Z","shell.execute_reply":"2024-11-01T18:26:53.306321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_LRCN_model()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:26:55.085196Z","iopub.execute_input":"2024-11-01T18:26:55.085584Z","iopub.status.idle":"2024-11-01T18:26:56.399029Z","shell.execute_reply.started":"2024-11-01T18:26:55.085547Z","shell.execute_reply":"2024-11-01T18:26:56.398144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_model(model, to_file = 'Suspicious_Human_Activity_LRCN_Model.png', show_shapes = True, show_layer_names = True)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:26:57.699240Z","iopub.execute_input":"2024-11-01T18:26:57.699941Z","iopub.status.idle":"2024-11-01T18:26:58.284445Z","shell.execute_reply.started":"2024-11-01T18:26:57.699902Z","shell.execute_reply":"2024-11-01T18:26:58.283594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nearly_stopping_callback = EarlyStopping(monitor = 'accuracy', patience = 10, mode = 'max', restore_best_weights = True)\n \n# Compile the model and specify loss function, optimizer and metrics to the model.\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'Adam', metrics = [\"accuracy\"])\n \n# Start training the model.\nmodel_training_history = model.fit(x = features_train, y = labels_train, epochs = 70, batch_size = 4 , shuffle = True, validation_split = 0.25, callbacks = [early_stopping_callback])","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:27:00.842700Z","iopub.execute_input":"2024-11-01T18:27:00.843595Z","iopub.status.idle":"2024-11-01T18:29:47.535951Z","shell.execute_reply.started":"2024-11-01T18:27:00.843552Z","shell.execute_reply":"2024-11-01T18:29:47.535134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save your Model.\nmodel.save(\"Suspicious_Human_Activity_Detection_LRCN_Model.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:44:39.736109Z","iopub.execute_input":"2024-11-01T18:44:39.736544Z","iopub.status.idle":"2024-11-01T18:44:39.782019Z","shell.execute_reply.started":"2024-11-01T18:44:39.736501Z","shell.execute_reply":"2024-11-01T18:44:39.780979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_metric(model_training_history, metric_name_1, metric_name_2, plot_name):\n    '''\n    This function will plot the metrics passed to it in a graph.\n    Args:\n        model_training_history: A history object containing a record of training and validation \n                                loss values and metrics values at successive epochs\n        metric_name_1:          The name of the first metric that needs to be plotted in the graph.\n        metric_name_2:          The name of the second metric that needs to be plotted in the graph.\n        plot_name:              The title of the graph.\n    '''\n    \n    # Get metric values using metric names as identifiers.\n    metric_value_1 = model_training_history.history[metric_name_1]\n    metric_value_2 = model_training_history.history[metric_name_2]\n    \n    # Construct a range object which will be used as x-axis (horizontal plane) of the graph.\n    epochs = range(len(metric_value_1))\n \n    # Plot the Graph.\n    plt.plot(epochs, metric_value_1, 'blue', label = metric_name_1)\n    plt.plot(epochs, metric_value_2, 'red', label = metric_name_2)\n \n    # Add title to the plot.\n    plt.title(str(plot_name))\n \n    # Add legend to the plot.\n    plt.legend()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:30:05.262797Z","iopub.execute_input":"2024-11-01T18:30:05.263702Z","iopub.status.idle":"2024-11-01T18:30:05.270464Z","shell.execute_reply.started":"2024-11-01T18:30:05.263660Z","shell.execute_reply":"2024-11-01T18:30:05.269502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(model_training_history, 'loss', 'val_loss', 'Total Loss vs Total Validation Loss')","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:30:07.281674Z","iopub.execute_input":"2024-11-01T18:30:07.282072Z","iopub.status.idle":"2024-11-01T18:30:07.592779Z","shell.execute_reply.started":"2024-11-01T18:30:07.282032Z","shell.execute_reply":"2024-11-01T18:30:07.591802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metric(model_training_history, 'accuracy', 'val_accuracy', 'Total Accuracy vs Total Validation Accuracy')","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:30:11.230666Z","iopub.execute_input":"2024-11-01T18:30:11.231056Z","iopub.status.idle":"2024-11-01T18:30:11.594646Z","shell.execute_reply.started":"2024-11-01T18:30:11.231017Z","shell.execute_reply":"2024-11-01T18:30:11.593603Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate Accuracy On Test Dataset\nacc = 0\nfor i in range(len(features_test)):\n  predicted_label = np.argmax(model.predict(np.expand_dims(features_test[i],axis =0))[0])\n  actual_label = np.argmax(labels_test[i])\n  if predicted_label == actual_label:\n      acc += 1\nacc = (acc * 100)/len(labels_test)\nprint(\"Accuracy =\",acc)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:30:13.492739Z","iopub.execute_input":"2024-11-01T18:30:13.493763Z","iopub.status.idle":"2024-11-01T18:30:22.196977Z","shell.execute_reply.started":"2024-11-01T18:30:13.493702Z","shell.execute_reply":"2024-11-01T18:30:22.196085Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_single_action(video_file_path, SEQUENCE_LENGTH):\n    '''\n    This function will perform single action recognition prediction on a video using the LRCN model.\n    Args:\n    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.\n    '''\n\n    # Initialize the VideoCapture object to read from the video file.\n    video_reader = cv2.VideoCapture(video_file_path)\n\n    # Get the width and height of the video.\n    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Declare a list to store video frames we will extract.\n    frames_list = []\n    \n    # Initialize a variable to store the predicted action being performed in the video.\n    predicted_class_name = ''\n\n    # Get the number of frames in the video.\n    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Calculate the interval after which frames will be added to the list.\n    skip_frames_window = max(int(video_frames_count/SEQUENCE_LENGTH),1)\n\n    # Iterating the number of times equal to the fixed length of sequence.\n    for frame_counter in range(SEQUENCE_LENGTH):\n\n        # Set the current frame position of the video.\n        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n\n        # Read a frame.\n        success, frame = video_reader.read() \n\n        # Check if frame is not read properly then break the loop.\n        if not success:\n            break\n\n        # Resize the Frame to fixed Dimensions.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        \n        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.\n        normalized_frame = resized_frame / 255\n        \n        # Appending the pre-processed frame into the frames list\n        frames_list.append(normalized_frame)\n\n    # Passing the  pre-processed frames to the model and get the predicted probabilities.\n    predicted_labels_probabilities = model.predict(np.expand_dims(frames_list, axis = 0))[0]\n\n    # Get the index of class with highest probability.\n    predicted_label = np.argmax(predicted_labels_probabilities)\n    \n    # Get the class name using the retrieved index.\n    predicted_class_name = CLASSES_LIST[predicted_label]\n    \n    # Display the predicted action along with the prediction confidence.\n    print(f'Action Predicted: {predicted_class_name}\\nConfidence: {predicted_labels_probabilities[predicted_label]}')\n        \n    # Release the VideoCapture object. \n    video_reader.release()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:30:28.281810Z","iopub.execute_input":"2024-11-01T18:30:28.282694Z","iopub.status.idle":"2024-11-01T18:30:28.292656Z","shell.execute_reply.started":"2024-11-01T18:30:28.282654Z","shell.execute_reply":"2024-11-01T18:30:28.291719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_single_action(\"/kaggle/input/fight-predict/fight.avi\",SEQUENCE_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:30:34.340913Z","iopub.execute_input":"2024-11-01T18:30:34.341319Z","iopub.status.idle":"2024-11-01T18:30:34.741977Z","shell.execute_reply.started":"2024-11-01T18:30:34.341274Z","shell.execute_reply":"2024-11-01T18:30:34.741080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_single_action(\"/kaggle/input/ruuning-predict/Predict_running.avi\",SEQUENCE_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:30:38.509950Z","iopub.execute_input":"2024-11-01T18:30:38.510992Z","iopub.status.idle":"2024-11-01T18:30:38.970436Z","shell.execute_reply.started":"2024-11-01T18:30:38.510949Z","shell.execute_reply":"2024-11-01T18:30:38.969515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_single_action(\"/kaggle/input/walking-predict/Predict_walking.avi\",SEQUENCE_LENGTH)","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:34:28.957516Z","iopub.execute_input":"2024-11-01T18:34:28.958474Z","iopub.status.idle":"2024-11-01T18:34:29.356048Z","shell.execute_reply.started":"2024-11-01T18:34:28.958433Z","shell.execute_reply":"2024-11-01T18:34:29.354955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predict_on_video(video_file_path, output_file_path, SEQUENCE_LENGTH):\n    '''\n    This function will perform action recognition on a video using the LRCN model.\n    Args:\n    video_file_path:  The path of the video stored in the disk on which the action recognition is to be performed.\n    output_file_path: The path where the ouput video with the predicted action being performed overlayed will be stored.\n    SEQUENCE_LENGTH:  The fixed number of frames of a video that can be passed to the model as one sequence.\n    '''\n\n    # Initialize the VideoCapture object to read from the video file.\n    video_reader = cv2.VideoCapture(video_file_path)\n\n    # Get the width and height of the video.\n    original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n    original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Initialize the VideoWriter Object to store the output video in the disk.\n    video_writer = cv2.VideoWriter(output_file_path, cv2.VideoWriter_fourcc(*'DIVX'), \n                                   video_reader.get(cv2.CAP_PROP_FPS), (original_video_width, original_video_height))\n\n    # Declare a queue to store video frames.\n    frames_queue = deque(maxlen = SEQUENCE_LENGTH)\n\n    # Initialize a variable to store the predicted action being performed in the video.\n    predicted_class_name = ''\n\n    # Iterate until the video is accessed successfully.\n    while video_reader.isOpened():\n\n        # Read the frame.\n        ok, frame = video_reader.read() \n        \n        # Check if frame is not read properly then break the loop.\n        if not ok:\n            break\n\n        # Resize the Frame to fixed Dimensions.\n        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n        \n        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.\n        normalized_frame = resized_frame / 255\n\n        # Appending the pre-processed frame into the frames list.\n        frames_queue.append(normalized_frame)\n\n        # Check if the number of frames in the queue are equal to the fixed sequence length.\n        if len(frames_queue) == SEQUENCE_LENGTH:\n\n            # Pass the normalized frames to the model and get the predicted probabilities.\n            predicted_labels_probabilities = model.predict(np.expand_dims(frames_queue, axis = 0))[0]\n\n            # Get the index of class with highest probability.\n            predicted_label = np.argmax(predicted_labels_probabilities)\n            \n             # Get the class name using the retrieved index.\n            predicted_class_name = CLASSES_LIST[predicted_label]\n\n        # Write predicted class name on top of the frame.\n        cv2.putText(frame, predicted_class_name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n        # Write The frame into the disk using the VideoWriter Object.\n        video_writer.write(frame)\n        \n    # Release the VideoCapture and VideoWriter objects.\n    video_reader.release()\n    video_writer.release()","metadata":{"execution":{"iopub.status.busy":"2024-11-01T18:43:14.305246Z","iopub.execute_input":"2024-11-01T18:43:14.305716Z","iopub.status.idle":"2024-11-01T18:43:14.317067Z","shell.execute_reply.started":"2024-11-01T18:43:14.305678Z","shell.execute_reply":"2024-11-01T18:43:14.316152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\npredict_on_video(\"Predict/Human-Activity.avi\",\"Human-Activity-Prediction.avi\",SEQUENCE_LENGTH)","metadata":{},"execution_count":null,"outputs":[]}]}